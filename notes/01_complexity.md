# Types of Time Complexity Functions

Understanding common time complexities in algorithm analysis:

| Notation | Name | Description |
|-----------|------|--------------|
| **O(1)** | Constant | Execution time does not depend on the input size. |
| **O(log n)** | Logarithmic | Time increases slowly as input size grows (e.g., binary search). |
| **O(n)** | Linear | Time grows directly with input size. |
| **O(n¬≤)** | Quadratic | Time grows proportional to the square of input size (e.g., bubble sort). |
| **O(n¬≥)** | Cubic | Time grows proportional to the cube of input size (e.g., triple nested loops). |
| **O(2‚Åø)** | Exponential | Time doubles with each additional input element (e.g., recursive Fibonacci). |

---

### üîç Quick Summary

| Growth Rate | Example Algorithm | Performance |
|--------------|------------------|--------------|
| **O(1)** | Accessing array element | üöÄ Fastest |
| **O(log n)** | Binary Search | ‚ö° Very Efficient |
| **O(n)** | Linear Search | ‚öôÔ∏è Moderate |
| **O(n¬≤)** | Bubble Sort | üê¢ Slow for large data |
| **O(n¬≥)** | Matrix Multiplication | üêå Slower |
| **O(2‚Åø)** | Recursive Fibonacci | üíÄ Extremely Slow |

---

üìò *Tip:* When analyzing algorithms, always aim for **O(log n)** or **O(n)** if possible!

---

## Asymptotic Notations

| Notation | Meaning | Description |
|-----------|----------|-------------|
| **Big O (O)** | Upper Bound | Represents the worst-case growth rate of an algorithm. |
| **Big Omega (Œ©)** | Lower Bound | Represents the best-case growth rate of an algorithm. |
| **Theta (Œò)** | Tight Bound | Represents the average or exact growth rate (when both upper and lower bounds are the same). |

---

## üìò Master Theorem ‚Äî Recurrence Relations and Time Complexities

The **Master Theorem** helps determine the time complexity of divide-and-conquer algorithms expressed as recurrences.

A general recurrence looks like this:

> **T(n) = a¬∑T(n/b) + f(n)**
> where
> - `a` ‚Üí number of subproblems
> - `b` ‚Üí factor by which the problem size is divided
> - `f(n)` ‚Üí extra work done outside recursion (like merging or partitioning)

---

### üîπ Common Recurrence Relations and Their Complexities

| Recurrence Relation | Description | Time Complexity |
|----------------------|--------------|-----------------|
| **T(n) = T(n - 1) + 1** | Reduces problem size by 1 each time (like linear recursion) | **O(n)** |
| **T(n) = T(n/2) + 1** | Divide problem in half each time (binary recursion) | **O(log n)** |
| **T(n) = 2T(n/2) + 1** | Two subproblems of half size each (like merge sort) | **O(n)** |
| **T(n) = 2T(n/2) + n** | Two halves + linear merge work (merge sort exactly) | **O(n log n)** |
| **T(n) = 3T(n/2) + n** | Three subproblems, half size each | **O(n^log‚ÇÇ3)** ‚âà **O(n^1.585)** |
| **T(n) = T(n/2) + n** | One subproblem + linear extra work | **O(n)** |
| **T(n) = 2T(n/2) + n¬≤** | Two subproblems + quadratic combination | **O(n¬≤)** |
| **T(n) = T(n - 1) + n** | Linear reduction + linear work each step | **O(n¬≤)** |
| **T(n) = 2T(n - 1) + 1** | Each call doubles recursive work | **O(2‚Åø)** |
| **T(n) = T(n/2) + log n** | Divide by half + logarithmic work | **O(log¬≤ n)** |
| **T(n) = 2T(n/2) + ‚àön** | Two halves + root-n extra work | **O(n)** (since ‚àön < n^log‚ÇÇ2 = n) |

---

### üß† Quick Intuition

| Pattern | Growth | Example Algorithm |
|----------|---------|------------------|
| **T(n) = T(n - 1) + 1** | Linear | Linear recursion (simple loops, factorial) |
| **T(n) = T(n/2) + 1** | Logarithmic | Binary Search |
| **T(n) = 2T(n/2) + n** | Linearithmic | Merge Sort, Quick Sort (avg) |
| **T(n) = 2T(n/2) + n¬≤** | Quadratic | Complex merging operations |
| **T(n) = T(n - 1) + n** | Quadratic | Insertion Sort |
| **T(n) = 2T(n - 1) + 1** | Exponential | Recursive Fibonacci |
| **T(n) = T(n/2) + log n** | Log¬≤ growth | Divide-and-conquer with log work |
| **T(n) = 3T(n/2) + n** | Superlinear | Strassen‚Äôs Matrix Multiplication |

---

üìò *Tip:*
When comparing recurrences, focus on whether the **subproblem count (a)** or the **work outside recursion (f(n))** dominates.
The Master Theorem simplifies identifying which one controls the total growth.

